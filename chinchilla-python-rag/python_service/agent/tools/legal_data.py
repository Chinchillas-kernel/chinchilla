#!/usr/bin/env python3
"""Utilities for chunking and embedding legal documents into ChromaDB."""
from __future__ import annotations

import argparse
import json
import os
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

import chromadb
from bs4 import BeautifulSoup
from chromadb.config import Settings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_upstage import UpstageEmbeddings
from tqdm import tqdm

try:
    from app.config import settings
except Exception as exc:
    raise RuntimeError(
        "Failed to import app.config.settings. Ensure PYTHONPATH is set."
    ) from exc


@dataclass
class LegalRecord:
    """ë²•ë¥  ë¬¸ì„œ ë ˆì½”ë“œ"""

    doc_id: str
    text: str
    metadata: Dict[str, Any]


@dataclass
class ChunkRecord:
    """ì²­í¬ ë ˆì½”ë“œ"""

    chunk_id: str
    doc_id: str
    text: str
    metadata: Dict[str, Any]
    embedding: Optional[Sequence[float]] = None


# ë²•ë¥  ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
LEGAL_CATEGORIES = {
    "ê²½ì œì§€ì›": ["ê¸°ì´ˆì—°ê¸ˆë²•", "êµ­ë¯¼ì—°ê¸ˆë²•", "ê¸´ê¸‰ë³µì§€ì§€ì›ë²•"],
    "ì˜ë£Œê±´ê°•": ["ë…¸ì¸ì¥ê¸°ìš”ì–‘ë³´í—˜ë²•", "ì˜ë£Œê¸‰ì—¬ë²•", "êµ­ë¯¼ê±´ê°•ë³´í—˜ë²•", "ì¹˜ë§¤ê´€ë¦¬ë²•"],
    "ë³µì§€ì„œë¹„ìŠ¤": [
        "ë…¸ì¸ë³µì§€ë²•",
        "ì‚¬íšŒë³´ì¥ê¸°ë³¸ë²•",
        "ê³ ë ¹ì¹œí™”ì‚°ì—…ì§„í¥ë²•",
        "ì¥ì• ì¸ë³µì§€ë²•",
    ],
    "ì£¼ê±°": ["ì£¼ê±°ê¸‰ì—¬ë²•", "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•"],
    "ë²•ë¥ ê¶Œë¦¬": ["ë¯¼ë²•"],
}


def _ensure_dir(path: Path) -> Path:
    """ë””ë ‰í† ë¦¬ ìƒì„±"""
    path.mkdir(parents=True, exist_ok=True)
    return path


def _classify_law(law_name: str) -> str:
    """ë²•ë¥  ì´ë¦„ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    for category, laws in LEGAL_CATEGORIES.items():
        for law in laws:
            if law in law_name:
                return category
    return "ê¸°íƒ€"


def _extract_law_info(law_name: str) -> Dict[str, str]:
    """ë²•ë¥ ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
    info = {"short_name": law_name}

    # ì•½ì¹­ ìƒì„±
    if "ë…¸ì¸" in law_name:
        info["target"] = "ë…¸ì¸"
    if "ì¥ì• ì¸" in law_name:
        info["target"] = "ì¥ì• ì¸"

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = []
    keyword_map = {
        "ì—°ê¸ˆ": ["ì—°ê¸ˆ", "ê¸‰ì—¬", "ìˆ˜ê¸‰"],
        "ë³µì§€": ["ë³µì§€", "ì„œë¹„ìŠ¤", "ì‹œì„¤"],
        "ì˜ë£Œ": ["ì˜ë£Œ", "ê±´ê°•", "ìš”ì–‘"],
        "ì£¼íƒ": ["ì£¼íƒ", "ì£¼ê±°", "ì„ëŒ€"],
        "ë³´í—˜": ["ë³´í—˜", "ë³´ì¥"],
    }

    for key, terms in keyword_map.items():
        if any(term in law_name for term in terms):
            keywords.append(key)

    info["keywords"] = ",".join(keywords) if keywords else ""

    return info


def _clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if not text:
        return ""

    # ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\n\s*\n", "\n\n", text)

    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    text = text.replace("\xa0", " ")
    text = text.replace("\u3000", " ")

    return text.strip()


def _normalize_metadata(meta: Dict[str, Any]) -> Dict[str, Any]:
    """ë©”íƒ€ë°ì´í„° ì •ê·œí™”"""
    cleaned: Dict[str, Any] = {}
    for key, value in meta.items():
        if value is None:
            continue
        if isinstance(value, str) and not value.strip():
            continue
        cleaned[key] = value
    return cleaned


def load_legal_pdfs(pdf_dir: Path, limit: int = 0) -> List[LegalRecord]:
    """
    PDF ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ë²•ë¥  ë¬¸ì„œ ë¡œë“œ

    Args:
        pdf_dir: PDF íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        limit: ë¡œë“œí•  íŒŒì¼ ìˆ˜ ì œí•œ (0 = ì „ì²´)

    Returns:
        LegalRecord ë¦¬ìŠ¤íŠ¸
    """
    if not pdf_dir.exists():
        raise FileNotFoundError(f"PDF directory not found: {pdf_dir}")

    pdf_files = sorted(pdf_dir.glob("*.pdf"))
    if not pdf_files:
        raise FileNotFoundError(f"No PDF files found in {pdf_dir}")

    if limit > 0:
        pdf_files = pdf_files[:limit]

    print(f"[INFO] Found {len(pdf_files)} PDF files")

    legal_records: List[LegalRecord] = []

    for pdf_file in tqdm(pdf_files, desc="Loading PDFs"):
        try:
            # PDF ë¡œë“œ
            loader = PyPDFLoader(str(pdf_file))
            pages = loader.load()

            if not pages:
                print(f"[WARN] Empty PDF: {pdf_file.name}")
                continue

            # ë²•ë¥ ëª… ì¶”ì¶œ
            law_name = pdf_file.stem
            law_info = _extract_law_info(law_name)
            category = _classify_law(law_name)

            # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
            full_text = "\n\n".join([page.page_content for page in pages])
            full_text = _clean_text(full_text)

            if not full_text:
                print(f"[WARN] No text extracted from {pdf_file.name}")
                continue

            # ë¬¸ì„œ ID ìƒì„±
            doc_id = f"law_{len(legal_records):04d}"

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = _normalize_metadata(
                {
                    "doc_id": doc_id,
                    "law_name": law_name,
                    "law_short_name": law_info.get("short_name", law_name),
                    "category": category,
                    "keywords": law_info.get("keywords", ""),
                    "target": law_info.get("target", ""),
                    "total_pages": len(pages),
                    "file_name": pdf_file.name,
                    "source": "legal_pdf",
                }
            )

            legal_records.append(
                LegalRecord(doc_id=doc_id, text=full_text, metadata=metadata)
            )

            print(
                f"[INFO] Loaded: {law_name} ({len(pages)} pages, {len(full_text)} chars)"
            )

        except Exception as e:
            print(f"[ERROR] Failed to load {pdf_file.name}: {e}")
            continue

    return legal_records


def dump_legal_texts(path: Path, records: Iterable[LegalRecord]) -> None:
    """ë²•ë¥  ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ JSONL íŒŒì¼ë¡œ ì €ì¥"""
    with path.open("w", encoding="utf-8") as handle:
        for record in records:
            payload = {
                "doc_id": record.doc_id,
                "text": record.text,
                "metadata": record.metadata,
            }
            json.dump(payload, handle, ensure_ascii=False)
            handle.write("\n")


def chunk_legal_docs(
    records: Sequence[LegalRecord],
    chunk_size: int,
    chunk_overlap: int,
) -> List[ChunkRecord]:
    """
    ë²•ë¥  ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 

    Args:
        records: ë²•ë¥  ë¬¸ì„œ ë ˆì½”ë“œë“¤
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ê²¹ì¹¨ í¬ê¸°

    Returns:
        ChunkRecord ë¦¬ìŠ¤íŠ¸
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\nì œ", "\n\n", "\n", ".", " ", ""],  # ë²•ë¥  ì¡°í•­ ê¸°ì¤€
    )

    chunk_records: List[ChunkRecord] = []

    for record in tqdm(records, desc="Chunking"):
        splits = splitter.split_text(record.text)

        if not splits:
            splits = [record.text]

        total_chunks = len(splits)

        for idx, chunk_text in enumerate(splits):
            chunk_id = f"{record.doc_id}#chunk_{idx}"

            # ë©”íƒ€ë°ì´í„° ë³µì‚¬ ë° í™•ì¥
            metadata = dict(record.metadata)
            metadata.update(
                {
                    "chunk_id": chunk_id,
                    "chunk_index": idx,
                    "chunk_count": total_chunks,
                    "chunk_size": len(chunk_text),
                }
            )

            chunk_records.append(
                ChunkRecord(
                    chunk_id=chunk_id,
                    doc_id=record.doc_id,
                    text=chunk_text,
                    metadata=_normalize_metadata(metadata),
                )
            )

    return chunk_records


def embed_and_ingest(
    chunks: Sequence[ChunkRecord],
    collection_name: str,
    db_path: Path,
    batch_size: int,
    reset_collection: bool,
) -> List[ChunkRecord]:
    """
    ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  ChromaDBì— ì €ì¥

    Args:
        chunks: ì²­í¬ ë ˆì½”ë“œë“¤
        collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
        db_path: ChromaDB ì €ì¥ ê²½ë¡œ
        batch_size: ì„ë² ë”© ë°°ì¹˜ í¬ê¸°
        reset_collection: ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì—¬ë¶€

    Returns:
        ì„ë² ë”©ì´ í¬í•¨ëœ ChunkRecord ë¦¬ìŠ¤íŠ¸
    """
    # API í‚¤ í™•ì¸
    api_key = os.getenv("UPSTAGE_API_KEY") or settings.upstage_api_key
    if not api_key:
        raise RuntimeError(
            "UPSTAGE_API_KEY not configured. Set environment variable or .env entry."
        )

    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = chromadb.PersistentClient(
        path=str(db_path.absolute()),
        settings=Settings(anonymized_telemetry=False),
    )

    # ì»¬ë ‰ì…˜ ë¦¬ì…‹
    if reset_collection:
        try:
            client.delete_collection(collection_name)
            print(f"[INFO] Deleted existing collection: {collection_name}")
        except Exception:
            pass

    # ì»¬ë ‰ì…˜ ìƒì„±
    collection = client.get_or_create_collection(name=collection_name)

    # Upstage Embeddings ì´ˆê¸°í™”
    embeddings_model = UpstageEmbeddings(
        api_key=api_key, model="solar-embedding-1-large"
    )

    # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ (ë¬¸ì„œ ID ê¸°ì¤€)
    unique_doc_ids = sorted({chunk.doc_id for chunk in chunks})
    for doc_id in unique_doc_ids:
        try:
            collection.delete(where={"doc_id": doc_id})
        except Exception:
            continue

    # ë°°ì¹˜ë¡œ ì„ë² ë”© ë° ì €ì¥
    updated_chunks: List[ChunkRecord] = []

    for start in tqdm(
        range(0, len(chunks), batch_size), desc="Embedding", unit="batch"
    ):
        batch = list(chunks[start : start + batch_size])
        texts = [item.text for item in batch]

        # ì„ë² ë”© ìƒì„±
        vectors = embeddings_model.embed_documents(texts)

        # ChromaDBì— ì €ì¥
        collection.upsert(
            ids=[item.chunk_id for item in batch],
            embeddings=vectors,
            metadatas=[item.metadata for item in batch],
            documents=texts,
        )

        # ê²°ê³¼ ì €ì¥
        for item, vector in zip(batch, vectors):
            updated_chunks.append(
                ChunkRecord(
                    chunk_id=item.chunk_id,
                    doc_id=item.doc_id,
                    text=item.text,
                    metadata=item.metadata,
                    embedding=vector,
                )
            )

    return updated_chunks


def dump_chunks(path: Path, chunks: Sequence[ChunkRecord]) -> None:
    """ì²­í¬ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    payload = [
        {
            "chunk_id": chunk.chunk_id,
            "doc_id": chunk.doc_id,
            "text": chunk.text,
            "metadata": chunk.metadata,
            "embedding": chunk.embedding,
        }
        for chunk in chunks
    ]
    with path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, ensure_ascii=False, indent=2)


def build_argument_parser() -> argparse.ArgumentParser:
    """CLI ì¸ì íŒŒì„œ ìƒì„±"""
    parser = argparse.ArgumentParser(
        description="Chunk and embed legal documents into ChromaDB."
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=0,
        help="Limit number of PDFs for debugging (0 = all)",
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=1000,
        help="Character length for each chunk",
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=200,
        help="Overlap size between chunks",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=16,
        help="Embedding batch size",
    )
    parser.add_argument(
        "--collection",
        type=str,
        default="elderly_legal",
        help="ChromaDB collection name",
    )
    parser.add_argument(
        "--db-dir",
        type=str,
        default=None,
        help="Override ChromaDB directory path",
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="Drop existing collection before ingesting",
    )
    return parser


def main(argv: Optional[Sequence[str]] = None) -> int:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = build_argument_parser()
    args = parser.parse_args(argv)

    # ê²½ë¡œ ì„¤ì •
    raw_dir = Path(settings.data_raw_dir)
    pdf_dir = raw_dir / "legal"
    processed_dir = _ensure_dir(Path("data/processed"))
    legal_texts_path = processed_dir / "legal_texts.jsonl"
    embedded_chunks_path = processed_dir / "legal_embedded_chunks.json"

    # ChromaDB ê²½ë¡œ
    if args.db_dir:
        db_dir = Path(args.db_dir)
    else:
        db_dir = Path(settings.chroma_dir).parent / "chroma_legal"

    _ensure_dir(db_dir)

    print("=" * 70)
    print("ğŸ“š ë²•ë¥  ë¬¸ì„œ ChromaDB ë¡œë”© íŒŒì´í”„ë¼ì¸")
    print("=" * 70)
    print(f"PDF ë””ë ‰í† ë¦¬: {pdf_dir}")
    print(f"ChromaDB ê²½ë¡œ: {db_dir}")
    print(f"ì»¬ë ‰ì…˜ëª…: {args.collection}")
    print(f"ì²­í¬ í¬ê¸°: {args.chunk_size} / ê²¹ì¹¨: {args.chunk_overlap}")
    print("=" * 70 + "\n")

    # 1. PDF ë¡œë“œ
    print("[STEP 1] Loading PDFs...")
    legal_records = load_legal_pdfs(pdf_dir, limit=args.limit)

    if not legal_records:
        print("[ERROR] No legal documents loaded; aborting.")
        return 1

    print(f"[SUCCESS] Loaded {len(legal_records)} legal documents\n")

    # 2. í…ìŠ¤íŠ¸ ì €ì¥
    print(f"[STEP 2] Writing normalized texts to {legal_texts_path}")
    dump_legal_texts(legal_texts_path, legal_records)
    print("[SUCCESS] Texts saved\n")

    # 3. ì²­í¬ ë¶„í• 
    print(f"[STEP 3] Chunking documents...")
    chunks = chunk_legal_docs(legal_records, args.chunk_size, args.chunk_overlap)
    print(f"[SUCCESS] Generated {len(chunks)} chunks\n")

    # 4. ì„ë² ë”© ë° ì €ì¥
    print(f"[STEP 4] Embedding and ingesting into ChromaDB...")
    embedded_chunks = embed_and_ingest(
        chunks,
        collection_name=args.collection,
        db_path=db_dir,
        batch_size=args.batch_size,
        reset_collection=args.reset,
    )
    print(f"[SUCCESS] Embedded and stored {len(embedded_chunks)} chunks\n")

    # 5. ì„ë² ë”© ë°ì´í„° ì €ì¥
    print(f"[STEP 5] Saving embedded chunks to {embedded_chunks_path}")
    dump_chunks(embedded_chunks_path, embedded_chunks)
    print("[SUCCESS] Embedded chunks saved\n")

    print("=" * 70)
    print("ğŸ‰ ë²•ë¥  ë¬¸ì„œ ë¡œë”© ì™„ë£Œ!")
    print("=" * 70)
    print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(legal_records)}")
    print(f"ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
    print(f"ChromaDB ê²½ë¡œ: {db_dir}")
    print(f"ì»¬ë ‰ì…˜ëª…: {args.collection}")
    print("=" * 70)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
