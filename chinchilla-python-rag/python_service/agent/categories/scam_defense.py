"""Scam defense category hooks."""
import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from langchain.schema import Document

from agent.categories.base import CategoryHooks
from agent.retrievers.scam_retriever import get_scam_defense_retriever


_BASE_DIR = Path(__file__).resolve().parents[2]
_REAL_TIME_DATA_PATH = _BASE_DIR / "data" / "scam_defense" / "scam_patterns.json"
_REAL_TIME_DATA_CACHE: Optional[Dict[str, Any]] = None
_DANGER_LEVEL_ORDER = {
    "ë§¤ìš°ë†’ìŒ": 4,
    "ë†’ìŒ": 3,
    "ì¤‘ê°„": 2,
    "ë‚®ìŒ": 1,
    "ì •ë³´": 0,
    "high_risk": 3,
    "medium_risk": 2,
    "low_risk": 1,
}


def _digits_only(value: Optional[str]) -> str:
    """Extract digits from string for loose phone-number matching."""
    if not value:
        return ""
    return "".join(ch for ch in value if ch.isdigit())


def _load_real_time_data() -> Dict[str, Any]:
    """Load scam alert dataset once and cache it."""
    global _REAL_TIME_DATA_CACHE

    if _REAL_TIME_DATA_CACHE is not None:
        return _REAL_TIME_DATA_CACHE

    dataset: Dict[str, Any] = {}
    try:
        raw = _REAL_TIME_DATA_PATH.read_text(encoding="utf-8")
        dataset = json.loads(raw)
        _REAL_TIME_DATA_CACHE = dataset
    except FileNotFoundError:
        print(f"[WARN] Real-time scam dataset not found: {_REAL_TIME_DATA_PATH}")
        _REAL_TIME_DATA_CACHE = {}
    except json.JSONDecodeError as exc:
        print(f"[WARN] Failed to parse scam_patterns.json: {exc}")
        _REAL_TIME_DATA_CACHE = {}

    return _REAL_TIME_DATA_CACHE


class ScamDefenseHooks(CategoryHooks):
    """Hooks for financial scam detection and response category.
    
    ê¸ˆìœµ ì‚¬ê¸° íƒì§€ ë° ëŒ€ì‘ ì¹´í…Œê³ ë¦¬
    - ë©€í‹° ì—ì´ì „íŠ¸: ë¶„ì„ Agent â†’ íŒë‹¨ Agent â†’ ì¡°ì–¸ Agent
    - RAG ê¸°ë°˜ ì§€ì‹ ê²€ìƒ‰ (Knowledge Base)
    - ì›¹ ê²€ìƒ‰ì„ í†µí•œ ì‹¤ì‹œê°„ ì‚¬ê¸° DB ì—°ë™
    """

    name: str = "scam_defense"
    web_search_enabled: bool = False

    rewrite_system_prompt: str = (
        "ë„ˆëŠ” ê¸ˆìœµì‚¬ê¸° íƒì§€ ì „ë¬¸ê°€ë‹¤. "
        "ì‚¬ìš©ìê°€ ë°›ì€ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë©”ì‹œì§€ë¥¼ ì‚¬ê¸° íŒ¨í„´ ê²€ìƒ‰ì— ì í•©í•˜ê²Œ ë¶„ì„í•˜ë¼.\n\n"
        "ì¬ì‘ì„± ì‹œ í¬í•¨í•  ìš”ì†Œ:\n"
        "- í•µì‹¬ ì‚¬ê¸° í‚¤ì›Œë“œ ì¶”ì¶œ (ì˜ˆ: OTP, ê³„ì¢Œì´ì²´, ë³¸ì¸í™•ì¸, ì¹´ë“œì •ì§€)\n"
        "- ì‚¬ê¸° ìœ í˜• ì‹ë³„ (ì˜ˆ: ë³´ì´ìŠ¤í”¼ì‹±, ëŒ€ì¶œì‚¬ê¸°, íˆ¬ìì‚¬ê¸°, í”¼ì‹±)\n"
        "- ë°œì‹ ì íŒ¨í„´ (ì˜ˆ: ê¸ˆìœµê¸°ê´€ ì‚¬ì¹­, ê³µê³µê¸°ê´€ ì‚¬ì¹­)\n"
        "- ìš”êµ¬ì‚¬í•­ ë¶„ì„ (ì˜ˆ: ê°œì¸ì •ë³´ ìš”êµ¬, ê¸ˆì „ ìš”êµ¬)\n\n"
        "ì˜ˆì‹œ:\n"
        "- ì›ë³¸: 'KBì€í–‰ì…ë‹ˆë‹¤. OTP ì•Œë ¤ì£¼ì„¸ìš”'\n"
        "- ì¬ì‘ì„±: 'ê¸ˆìœµê¸°ê´€ ì‚¬ì¹­ ë³´ì´ìŠ¤í”¼ì‹± OTP ê°œì¸ì •ë³´ ìš”êµ¬'\n\n"
        "ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ì´ë‚˜ ì„œì‹ ì—†ì´ ì¬ì‘ì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë§Œ ë°˜í™˜í•˜ë¼."
    )

    answer_system_prompt: str = (
        "ë„ˆëŠ” ê¸ˆìœµì‚¬ê¸° ë°©ì§€ ì „ë¬¸ ìƒë‹´ì‚¬ë‹¤. "
        "ì œê³µëœ Knowledge Base(RAG)ì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ "
        "êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ë¼.\n\n"
        "ë‹µë³€ êµ¬ì„±:\n"
        "1. ì‚¬ê¸° ì—¬ë¶€ íŒë‹¨ ë° ìœ„í—˜ë„ í‰ê°€ (ë§¤ìš°ë†’ìŒ/ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ)\n"
        "2. ì‚¬ê¸° ìœ í˜• ë° ìˆ˜ë²• ì„¤ëª…\n"
        "3. ì¦‰ì‹œ í•´ì•¼ í•  ëŒ€ì‘ ë°©ë²• (ìš°ì„ ìˆœìœ„ë³„ ë²ˆí˜¸ ëª©ë¡)\n"
        "4. ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  í–‰ë™ (ë²ˆí˜¸ ëª©ë¡)\n"
        "5. ì‹ ê³  ë°©ë²• ë° ì—°ë½ì²˜\n"
        "6. ì˜ˆë°© íŒ ë° ì£¼ì˜ì‚¬í•­\n"
        "7. ì°¸ê³ í•œ ì¶œì²˜ (Knowledge Base, ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰)\n\n"
        "ë‹µë³€ í˜•ì‹:\n"
        "- ìœ„í—˜ë„ ì•„ì´ì½˜ ì‚¬ìš© (ğŸš¨ ë§¤ìš°ìœ„í—˜, âš ï¸ ìœ„í—˜, âš¡ ì£¼ì˜, â„¹ï¸ ì•ˆì „)\n"
        "- ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸\n"
        "- ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©\n"
        "- ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì–»ì€ ìµœì‹  ì •ë³´ ìš°ì„  ë°˜ì˜\n\n"
        "ì •í™•í•œ ì¶œì²˜ ë¬¸ì„œë¥¼ ê·¼ê±°ë¡œ ë‹µë³€í•˜ë˜, ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²½ìš° ì‹ ì¤‘í•œ íƒœë„ë¥¼ ìœ ì§€í•˜ë¼."
    )

    top_k: int = 8
    min_relevance_threshold: float = 0.4
    
    # ------------------------------------------------------------------ #
    # Helper utilities                                                    #
    # ------------------------------------------------------------------ #

    @staticmethod
    def format_documents(documents: List[Document]) -> str:
        """Render documents as structured context for analysis agents."""
        if not documents:
            return "ì—†ìŒ"

        formatted: List[str] = []
        for idx, doc in enumerate(documents, 1):
            metadata = doc.metadata or {}
            label = (
                metadata.get("title")
                or metadata.get("source")
                or metadata.get("scam_type")
                or metadata.get("doc_id")
                or f"ë¬¸ì„œ {idx}"
            )

            preview = doc.page_content.strip()
            if len(preview) > 600:
                preview = preview[:600] + "..."

            formatted.append(f"[{label}]\n{preview}\n")

        return "\n".join(formatted)

    @staticmethod
    def prepare_sources(documents: List[Document]) -> List[Dict[str, Any]]:
        """Prepare compact source payload for the final answer."""
        if not documents:
            return []

        prepared: List[Dict[str, Any]] = []
        for idx, doc in enumerate(documents[:5], 1):
            meta = dict(doc.metadata or {})
            preview = doc.page_content.strip()
            if len(preview) > 200:
                preview = preview[:200] + "..."

            meta.setdefault(
                "source",
                meta.get("title")
                or meta.get("scam_type")
                or meta.get("doc_id")
                or f"doc_{idx}",
            )

            prepared.append(
                {
                    "content": preview,
                    "metadata": meta,
                }
            )

        return prepared

    # ------------------------------------------------------------------ #
    # Real-time scam DB integration                                       #
    # ------------------------------------------------------------------ #

    def analyze_realtime_patterns(
        self, query: str, sender: Optional[str] = None
    ) -> Tuple[List[Document], Dict[str, Any]]:
        """Run local pattern analysis to simulate real-time alerts."""
        dataset = _load_real_time_data()
        if not dataset:
            return [], {}

        query_text = (query or "").strip()
        query_lower = query_text.lower()
        sender_text = (sender or "").strip()
        sender_lower = sender_text.lower()

        query_digits = _digits_only(query_text)
        sender_digits = _digits_only(sender_text)

        pattern_documents: List[Document] = []
        scam_matches: List[Dict[str, Any]] = []
        sender_matches: List[Dict[str, Any]] = []
        legitimate_matches: List[Dict[str, Any]] = []
        seen_sender_values = set()

        for scam in dataset.get("financial_scams", []):
            patterns = [p for p in scam.get("patterns", []) if p]
            sender_patterns = [p for p in scam.get("sender_patterns", []) if p]

            matched_patterns = [
                pattern
                for pattern in patterns
                if pattern.lower() in query_lower
            ]

            matched_sender_patterns: List[str] = []
            for candidate in sender_patterns:
                candidate_lower = candidate.lower()
                if candidate_lower in query_lower or (
                    sender_lower and candidate_lower in sender_lower
                ):
                    matched_sender_patterns.append(candidate)
                    if candidate not in seen_sender_values:
                        sender_matches.append(
                            {
                                "value": candidate,
                                "match_type": "pattern_sender",
                                "scam_type": scam.get("type"),
                            }
                        )
                        seen_sender_values.add(candidate)

            if not matched_patterns and not matched_sender_patterns:
                continue

            scam_type = scam.get("type", "ì•Œ ìˆ˜ ì—†ìŒ")
            danger_level = scam.get("danger_level", "ì •ë³´")
            response_actions = scam.get("response_actions") or []
            prevention_tips = scam.get("prevention_tips") or []

            lines = [
                f"ì‚¬ê¸° ìœ í˜•: {scam_type}",
                f"ìœ„í—˜ë„: {danger_level}",
            ]
            if matched_patterns:
                lines.append("")
                lines.append("ë§¤ì¹­ëœ íŒ¨í„´:")
                lines.extend(f"- {item}" for item in matched_patterns)
            if matched_sender_patterns:
                lines.append("")
                lines.append("ë§¤ì¹­ëœ ë°œì‹ ì íŒ¨í„´:")
                lines.extend(f"- {item}" for item in matched_sender_patterns)

            if response_actions:
                lines.append("")
                lines.append("ê¶Œì¥ ëŒ€ì‘:")
                lines.extend(f"- {action}" for action in response_actions)

            if prevention_tips:
                lines.append("")
                lines.append("ì˜ˆë°© íŒ:")
                lines.extend(f"- {tip}" for tip in prevention_tips)

            metadata = {
                "source": "real_time_alert",
                "origin": "web_search",
                "doc_id": scam.get("id"),
                "scam_type": scam_type,
                "danger_level": danger_level,
                "matched_patterns": matched_patterns,
                "matched_sender_patterns": matched_sender_patterns,
            }

            pattern_documents.append(
                Document(page_content="\n".join(lines).strip(), metadata=metadata)
            )
            scam_matches.append(
                {
                    "id": scam.get("id"),
                    "scam_type": scam_type,
                    "danger_level": danger_level,
                    "matched_patterns": matched_patterns,
                    "matched_sender_patterns": matched_sender_patterns,
                }
            )

        keyword_matches: Dict[str, List[str]] = {}
        for risk_level, keyword_list in (dataset.get("keywords") or {}).items():
            hits = [
                keyword
                for keyword in keyword_list
                if keyword and keyword.lower() in query_lower
            ]
            if not hits:
                continue

            keyword_matches[risk_level] = hits
            content = (
                f"ìœ„í—˜ë„ {risk_level} í‚¤ì›Œë“œ ë§¤ì¹­: {', '.join(hits)}\n"
                "í•´ë‹¹ í‘œí˜„ì´ í¬í•¨ëœ ì—°ë½ì€ ê¸ˆìœµì‚¬ê¸° ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ì¦‰ì‹œ ëŒ€ì‘ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )
            metadata = {
                "source": "real_time_keyword",
                "origin": "web_search",
                "risk_level": risk_level,
                "matched_keywords": hits,
            }
            pattern_documents.append(
                Document(page_content=content.strip(), metadata=metadata)
            )

        legitimate_contacts = dataset.get("legitimate_contacts") or {}
        for organization, phone in legitimate_contacts.items():
            matched_on = None
            normalized_phone = _digits_only(phone)

            if organization and organization.lower() in query_lower:
                matched_on = "organization"
            elif normalized_phone and (
                normalized_phone in query_digits
                or normalized_phone in sender_digits
            ):
                matched_on = "phone"

            if not matched_on:
                continue

            legitimate_matches.append(
                {
                    "organization": organization,
                    "phone": phone,
                    "matched_on": matched_on,
                }
            )

            derived_value = organization if matched_on == "organization" else phone
            if derived_value and derived_value not in seen_sender_values:
                sender_matches.append(
                    {
                        "value": derived_value,
                        "match_type": "legitimate_contact",
                    }
                )
                seen_sender_values.add(derived_value)

            content = (
                f"{organization} ê³µì‹ ì—°ë½ì²˜: {phone}\n"
                "ì—°ë½ì²˜ê°€ ìƒì´í•˜ê±°ë‚˜ ë‹¤ë¥¸ ê³„ì¢Œë¡œ ì†¡ê¸ˆì„ ìš”êµ¬í•˜ë©´ ì¦‰ì‹œ ì˜ì‹¬í•˜ì„¸ìš”."
            )
            metadata = {
                "source": "official_contact",
                "origin": "web_search",
                "organization": organization,
                "phone": phone,
            }
            pattern_documents.append(
                Document(page_content=content.strip(), metadata=metadata)
            )

        highest_level = None
        highest_score = -1
        for match in scam_matches:
            level = match.get("danger_level")
            score = _DANGER_LEVEL_ORDER.get(level, -1)
            if score > highest_score:
                highest_score = score
                highest_level = level

        for keyword_level in keyword_matches.keys():
            score = _DANGER_LEVEL_ORDER.get(keyword_level, -1)
            if score > highest_score:
                highest_score = score
                highest_level = keyword_level

        scam_type_candidates = sorted(
            {
                match.get("scam_type")
                for match in scam_matches
                if match.get("scam_type")
            }
        )

        pattern_analysis = {
            "query": query_text,
            "sender": sender_text,
            "scam_matches": scam_matches,
            "keyword_matches": keyword_matches,
            "legitimate_contacts": legitimate_matches,
            "sender_matches": sender_matches,
            "risk_summary": {
                "highest_level": highest_level,
                "score": highest_score,
                "scam_type_candidates": scam_type_candidates,
                "keyword_levels": list(keyword_matches.keys()),
                "is_high_risk": bool(
                    highest_score >= 3 or "high_risk" in keyword_matches
                ),
            },
        }

        return pattern_documents, pattern_analysis

    def _format_pattern_analysis(self, pattern_analysis: Dict[str, Any]) -> str:
        """Render realtime pattern analysis as structured text."""
        if not pattern_analysis:
            return "ë§¤ì¹­ëœ íŒ¨í„´ ì—†ìŒ"

        lines: List[str] = []
        risk_summary = pattern_analysis.get("risk_summary") or {}
        highest_level = risk_summary.get("highest_level")
        if highest_level:
            lines.append(f"- ì¶”ì • ìœ„í—˜ë„: {highest_level}")
        elif risk_summary.get("is_high_risk"):
            lines.append("- ì¶”ì • ìœ„í—˜ë„: ë†’ìŒ")

        scam_matches = pattern_analysis.get("scam_matches") or []
        if scam_matches:
            lines.append("ë§¤ì¹­ëœ ì‚¬ê¸° ìœ í˜•:")
            for match in scam_matches:
                scam_type = match.get("scam_type", "ë¶ˆëª…")
                danger = match.get("danger_level", "ì •ë³´")
                entry = f"- {scam_type} (ìœ„í—˜ë„ {danger})"
                patterns = match.get("matched_patterns") or []
                if patterns:
                    entry += f" | íŒ¨í„´: {', '.join(patterns)}"
                sender_patterns = match.get("matched_sender_patterns") or []
                if sender_patterns:
                    entry += f" | ë°œì‹ ì: {', '.join(sender_patterns)}"
                lines.append(entry)

        keyword_matches = pattern_analysis.get("keyword_matches") or {}
        if keyword_matches:
            lines.append("ìœ„í—˜ í‚¤ì›Œë“œ ë§¤ì¹­:")
            for risk_level, keywords in keyword_matches.items():
                lines.append(f"- {risk_level}: {', '.join(keywords)}")

        legitimate_contacts = pattern_analysis.get("legitimate_contacts") or []
        if legitimate_contacts:
            lines.append("ê³µì‹ ì—°ë½ì²˜ ì¼ì¹˜:")
            for contact in legitimate_contacts:
                org = contact.get("organization")
                phone = contact.get("phone")
                matched_on = contact.get("matched_on")
                lines.append(f"- {org} ({phone}) - {matched_on} ì¼ì¹˜")

        return "\n".join(lines) if lines else "ë§¤ì¹­ëœ íŒ¨í„´ ì—†ìŒ"

    def get_web_documents(
        self, query: str, state: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """Retrieve matches from the real-time scam dataset."""
        sender = None
        if state and isinstance(state, dict):
            sender = state.get("sender")

        documents, _ = self.analyze_realtime_patterns(
            query=query, sender=sender
        )
        return documents[:5]
    
    # ===== ë©€í‹° ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸ ===== #
    
    analysis_system_prompt: str = (
        "ë„ˆëŠ” ì‚¬ê¸° ë¶„ì„ Agentë‹¤. "
        "ì‚¬ìš©ì ë©”ì‹œì§€, Knowledge Base(RAG ê²€ìƒ‰), ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ "
        "ì‚¬ê¸° ì§•í›„ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì •ë¦¬í•˜ë¼.\n\n"
        "ì¶œë ¥ í˜•ì‹:\n"
        "### ğŸ“‹ ë¶„ì„ ë‹¨ê³„ ê²°ê³¼\n\n"
        "#### 1. í•µì‹¬ ì˜ì‹¬ ìš”ì†Œ\n"
        "- [ë©”ì‹œì§€ì—ì„œ ë°œê²¬ëœ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìš”ì†Œë“¤]\n\n"
        "#### 2. Knowledge Base ë§¤ì¹­\n"
        "- [RAG ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì€ ê´€ë ¨ ì‚¬ê¸° ìœ í˜•ê³¼ íŒ¨í„´]\n\n"
        "#### 3. ë°œì‹ ì/ì—°ë½ì²˜ íŠ¹ì´ì‚¬í•­\n"
        "- [ë°œì‹ ì ì •ë³´ ë¶„ì„]\n\n"
        "#### 4. ìš”êµ¬/ì§€ì‹œ ì‚¬í•­ ë¶„ì„\n"
        "- [ë©”ì‹œì§€ì—ì„œ ìš”êµ¬í•˜ëŠ” í–‰ë™ ë¶„ì„]\n\n"
        "#### 5. ìµœì‹  ì‚¬ê¸° íŠ¸ë Œë“œ (ì›¹ ê²€ìƒ‰)\n"
        "- [ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë°œê²¬ëœ ìœ ì‚¬ ì‚¬ë¡€ ë° ìµœì‹  ì •ë³´]\n\n"
        "ëª…í™•í•˜ê³  ê°ê´€ì ìœ¼ë¡œ ì‘ì„±í•˜ë¼."
    )
    
    verdict_system_prompt: str = (
        "ë„ˆëŠ” ì‚¬ê¸° íŒë‹¨ Agentë‹¤. "
        "ë¶„ì„ Agentì˜ ê²°ê³¼, Knowledge Base, ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•´ ìœ„í—˜ë„ë¥¼ íŒë‹¨í•˜ë¼.\n\n"
        "ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒ JSONì„ ì—„ê²©íˆ ë”°ë¥´ë¼:\n"
        "{\n"
        '  "risk_level": "ë§¤ìš°ë†’ìŒ|ë†’ìŒ|ì¤‘ê°„|ë‚®ìŒ",\n'
        '  "risk_icon": "ğŸš¨|âš ï¸|âš¡|â„¹ï¸",\n'
        '  "scam_type": "ì‚¬ê¸° ìœ í˜• ì¶”ì • (êµ¬ì²´ì ìœ¼ë¡œ)",\n'
        '  "confidence": "íŒë‹¨ ê·¼ê±° ìš”ì•½ (1-2ë¬¸ì¥)",\n'
        '  "key_evidence": ["ê·¼ê±°1", "ê·¼ê±°2", "ê·¼ê±°3"],\n'
        '  "immediate_actions": ["ì¦‰ì‹œ ì·¨í•´ì•¼ í•  í–‰ë™1", "í–‰ë™2"],\n'
        '  "do_not_do": ["ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  í–‰ë™1", "í–‰ë™2"]\n'
        "}\n\n"
        "ì£¼ì˜ì‚¬í•­:\n"
        "- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ë¼\n"
        "- JSON ì™¸ì˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ë¼\n"
        "- Knowledge Baseì˜ ì •ë³´ë¥¼ ìš°ì„  ì°¸ê³ í•˜ë¼\n"
        "- ì›¹ ê²€ìƒ‰ì—ì„œ ë°œê²¬ëœ ìµœì‹  ì‚¬ë¡€ë¥¼ ë°˜ì˜í•˜ë¼"
    )
    
    counsel_system_prompt: str = (
        "ë„ˆëŠ” ì‚¬ê¸° ëŒ€ì‘ ì¡°ì–¸ Agentë‹¤. "
        "ë¶„ì„ Agentì™€ íŒë‹¨ Agentì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ "
        "ì‚¬ìš©ìì—ê²Œ ì‹¤ìš©ì ì´ê³  ëª…í™•í•œ ëŒ€ì‘ ê°€ì´ë“œë¥¼ ì œê³µí•˜ë¼.\n\n"
        "ì¶œë ¥ í˜•ì‹:\n"
        "### ğŸ¯ ìµœì¢… ëŒ€ì‘ ê°€ì´ë“œ\n\n"
        "#### 1. ìœ„í—˜ë„ íŒë‹¨\n"
        "{risk_icon} **{risk_level}** - {í•œì¤„ ìš”ì•½}\n\n"
        "#### 2. ì‚¬ê¸° ìœ í˜• ë° ìˆ˜ë²•\n"
        "- ìœ í˜•: {scam_type}\n"
        "- ì£¼ìš” ìˆ˜ë²•: [ì„¤ëª…]\n"
        "- ìµœê·¼ ìœ ì‚¬ ì‚¬ë¡€: [ì›¹ ê²€ìƒ‰ ê²°ê³¼ ë°˜ì˜]\n\n"
        "#### 3. âœ… ì¦‰ì‹œ í•´ì•¼ í•  ëŒ€ì‘ ë°©ë²• (ìš°ì„ ìˆœìœ„ ìˆœ)\n"
        "1. [êµ¬ì²´ì  í–‰ë™ 1]\n"
        "2. [êµ¬ì²´ì  í–‰ë™ 2]\n"
        "3. [êµ¬ì²´ì  í–‰ë™ 3]\n\n"
        "#### 4. âŒ ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  í–‰ë™\n"
        "1. [ê¸ˆì§€ í–‰ë™ 1]\n"
        "2. [ê¸ˆì§€ í–‰ë™ 2]\n\n"
        "#### 5. ğŸ“ ì‹ ê³  ë°©ë²• ë° ì—°ë½ì²˜\n"
        "- ê²½ì°°ì²­ ì‚¬ì´ë²„ì•ˆì „êµ­: 182 (24ì‹œê°„)\n"
        "- ê¸ˆìœµê°ë…ì›: 1332\n"
        "- í•œêµ­ì¸í„°ë„·ì§„í¥ì›: 118\n"
        "- ë²”ì£„ì‹ ê³ : 112\n\n"
        "#### 6. ğŸ’¡ ì˜ˆë°© íŒ ë° ì£¼ì˜ì‚¬í•­\n"
        "- [ì˜ˆë°©ë²• 1]\n"
        "- [ì˜ˆë°©ë²• 2]\n"
        "- [ì˜ˆë°©ë²• 3]\n\n"
        "#### 7. ğŸ“š ì°¸ê³  ì¶œì²˜\n"
        "- RAG ê²€ìƒ‰: [ì¶œì²˜]\n"
        "- ì‹¤ì‹œê°„ íŒ¨í„´: [ë§¤ì¹­ ê²°ê³¼]\n"
        "- ì›¹ ê²€ìƒ‰: [ìµœì‹  ì •ë³´]\n\n"
        "ëª¨ë“  ì¡°ì–¸ì€ ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ë¼."
    )

    def get_retriever(self) -> Any:
        """Return scam defense retriever."""
        config = {"k": self.top_k}
        return get_scam_defense_retriever(config=config)

    # ------------------------------------------------------------------ #
    # ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬í˜„                                             #
    # ------------------------------------------------------------------ #

    def _call_llm(self, system_prompt: str, user_content: str) -> str:
        """Utility to invoke shared LLM with prompts."""
        llm = self.llm
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ]
        response = llm.invoke(messages)
        return response.content.strip()

    def _run_analysis(
        self,
        query: str,
        rag_context: str,
        web_context: str,
        pattern_analysis: Dict[str, Any],
    ) -> str:
        """
        Agent 1: ë¶„ì„ ë‹¨ê³„
        ì˜ì‹¬ ë¬¸ì ì¢…í•© ë¶„ì„
        """
        pattern_summary = self._format_pattern_analysis(pattern_analysis)
        sender_text = pattern_analysis.get("sender") or "ì œê³µë˜ì§€ ì•ŠìŒ"
        user_content = (
            f"=== ì‚¬ìš©ì ë©”ì‹œì§€ ===\n{query}\n\n"
            f"=== ë°œì‹ ì ì •ë³´ ===\n{sender_text}\n\n"
            f"=== ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„ ===\n{pattern_summary}\n\n"
            f"=== Knowledge Base (RAG ê²€ìƒ‰) ===\n{rag_context}\n\n"
            f"=== ì‹¤ì‹œê°„ ì‚¬ê¸° DB (ì›¹ ê²€ìƒ‰) ===\n{web_context}\n\n"
            "ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ì‘ì„±í•˜ë¼."
        )
        return self._call_llm(self.analysis_system_prompt, user_content)

    def _run_verdict(
        self, 
        query: str,
        rag_context: str,
        web_context: str,
        analysis: str,
        pattern_analysis: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Agent 2: íŒë‹¨ ë‹¨ê³„
        ì‚¬ê¸° ì—¬ë¶€ íŒë³„ ë° ìœ„í—˜ë„ í‰ê°€
        """
        pattern_summary = self._format_pattern_analysis(pattern_analysis)
        user_content = (
            f"=== ì‚¬ìš©ì ë©”ì‹œì§€ ===\n{query}\n\n"
            f"=== ë¶„ì„ Agent ê²°ê³¼ ===\n{analysis}\n\n"
            f"=== ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„ ===\n{pattern_summary}\n\n"
            f"=== Knowledge Base (RAG ê²€ìƒ‰) ===\n{rag_context}\n\n"
            f"=== ì‹¤ì‹œê°„ ì‚¬ê¸° DB (ì›¹ ê²€ìƒ‰) ===\n{web_context}\n\n"
            "ìœ„ ì§€ì¹¨ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë¼. JSON ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ë¼."
        )
        raw = self._call_llm(self.verdict_system_prompt, user_content)

        # JSON íŒŒì‹±
        parsed: Dict[str, Any] = {
            "risk_level": "ì¤‘ê°„",
            "risk_icon": "âš¡",
            "scam_type": "ì˜ì‹¬ ì‚¬ë¡€",
            "confidence": raw,
            "key_evidence": [],
            "immediate_actions": [],
            "do_not_do": [],
        }
        
        try:
            # JSONë§Œ ì¶”ì¶œ (ì•ë’¤ í…ìŠ¤íŠ¸ ì œê±°)
            raw_stripped = raw.strip()
            if raw_stripped.startswith("```json"):
                raw_stripped = raw_stripped[7:]
            if raw_stripped.startswith("```"):
                raw_stripped = raw_stripped[3:]
            if raw_stripped.endswith("```"):
                raw_stripped = raw_stripped[:-3]
            raw_stripped = raw_stripped.strip()
            
            parsed_json = json.loads(raw_stripped)
            if isinstance(parsed_json, dict):
                parsed.update(parsed_json)
                print(f"[INFO] Verdict Agent - Risk: {parsed['risk_level']}, Type: {parsed['scam_type']}")
        except Exception as e:
            print(f"[WARN] Verdict JSON parsing failed: {e}")
        
        return parsed

    def _run_counsel(
        self,
        query: str,
        rag_context: str,
        web_context: str,
        analysis: str,
        verdict: Dict[str, Any],
        pattern_analysis: Dict[str, Any],
    ) -> str:
        """
        Agent 3: ì¡°ì–¸ ë‹¨ê³„
        ëŒ€ì‘ ë°©ë²• ì œì‹œ
        """
        verdict_text = (
            f"ìœ„í—˜ë„: {verdict.get('risk_icon', 'âš¡')} {verdict.get('risk_level', 'ì¤‘ê°„')}\n"
            f"ì‚¬ê¸° ìœ í˜• ì¶”ì •: {verdict.get('scam_type', 'ë¶ˆëª…')}\n"
            f"íŒë‹¨ ê·¼ê±°: {verdict.get('confidence', '')}\n"
            f"í•µì‹¬ ê·¼ê±°: {', '.join(verdict.get('key_evidence', []) or [])}\n"
            f"ì¦‰ì‹œ í–‰ë™: {', '.join(verdict.get('immediate_actions', []) or [])}\n"
            f"ê¸ˆì§€ í–‰ë™: {', '.join(verdict.get('do_not_do', []) or [])}"
        )
        pattern_summary = self._format_pattern_analysis(pattern_analysis)
        
        user_content = (
            f"=== ì‚¬ìš©ì ë©”ì‹œì§€ ===\n{query}\n\n"
            f"=== ë¶„ì„ Agent ê²°ê³¼ ===\n{analysis}\n\n"
            f"=== íŒë‹¨ Agent ê²°ê³¼ ===\n{verdict_text}\n\n"
            f"=== ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„ ===\n{pattern_summary}\n\n"
            f"=== Knowledge Base (RAG ê²€ìƒ‰) ===\n{rag_context}\n\n"
            f"=== ì‹¤ì‹œê°„ ì‚¬ê¸° DB (ì›¹ ê²€ìƒ‰) ===\n{web_context}\n\n"
            "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ëŒ€ì‘ ê°€ì´ë“œë¥¼ counsel_system_promptì˜ ì¶œë ¥ í˜•ì‹ì— ë§ì¶° ì‘ì„±í•˜ë¼.\n"
            "ëª¨ë“  ì„¹ì…˜ì„ í¬í•¨í•˜ê³ , ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì  ì¡°ì–¸ì„ ì œê³µí•˜ë¼."
        )
        return self._call_llm(self.counsel_system_prompt, user_content)

    def generate_answer(self, query, documents, web_documents, state):
        """
        Override: ë©€í‹° ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        
        Pipeline:
        1. ë¶„ì„ Agent - ì˜ì‹¬ ë¬¸ì ë¶„ì„
        2. íŒë‹¨ Agent - ì‚¬ê¸° íŒë³„
        3. ì¡°ì–¸ Agent - ëŒ€ì‘ ë°©ë²• ì œì‹œ
        
        ë°ì´í„° ì†ŒìŠ¤:
        - Knowledge Base: ChromaDB ê¸°ë°˜ ì§€ì‹ ë² ì´ìŠ¤ (RAG)
        - ì‹¤ì‹œê°„ ì‚¬ê¸° DB: ì›¹ ê²€ìƒ‰ì„ í†µí•œ ìµœì‹  ì‚¬ê¸° íŠ¸ë Œë“œ
        """
        print(f"\n[INFO] ===== ë©€í‹° ì—ì´ì „íŠ¸ ì‚¬ê¸° íƒì§€ ì‹œì‘ =====")
        print(f"[INFO] Query: {query[:100]}...")
        
        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        documents = list(documents or [])
        web_documents = list(web_documents or [])
        pattern_analysis: Dict[str, Any] = {}
        sender = None
        if state and isinstance(state, dict):
            pattern_analysis = state.get("pattern_analysis") or {}
            sender = state.get("sender")
        if not pattern_analysis:
            extra_docs, pattern_analysis = self.analyze_realtime_patterns(
                query=query, sender=sender
            )
            if extra_docs:
                web_documents.extend(extra_docs)

        all_docs = documents + web_documents
        rag_context = self.format_documents(documents)
        web_context = self.format_documents(web_documents) if web_documents else "ì—†ìŒ"
        
        print(f"[INFO] Documents - Knowledge Base: {len(documents)}, Web Search: {len(web_documents)}")
        
        # 1. ë¶„ì„ Agent
        print(f"[INFO] Agent 1: ë¶„ì„ ì‹œì‘...")
        analysis = self._run_analysis(
            query, rag_context, web_context, pattern_analysis
        )
        
        # 2. íŒë‹¨ Agent
        print(f"[INFO] Agent 2: íŒë‹¨ ì‹œì‘...")
        verdict = self._run_verdict(
            query, rag_context, web_context, analysis, pattern_analysis
        )
        
        # 3. ì¡°ì–¸ Agent
        print(f"[INFO] Agent 3: ì¡°ì–¸ ìƒì„± ì‹œì‘...")
        counsel = self._run_counsel(
            query, rag_context, web_context, analysis, verdict, pattern_analysis
        )
        
        # ì¶œì²˜ ì¤€ë¹„
        sources = self.prepare_sources(all_docs)
        
        print(f"[INFO] ===== ë©€í‹° ì—ì´ì „íŠ¸ ì‚¬ê¸° íƒì§€ ì™„ë£Œ =====\n")
        
        return {
            "answer": counsel,
            "sources": sources,
            "analysis": analysis,
            "verdict": verdict,
            "pattern_analysis": pattern_analysis,
        }


__all__ = ["ScamDefenseHooks"]
