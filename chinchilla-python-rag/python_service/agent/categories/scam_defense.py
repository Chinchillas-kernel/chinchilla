"""Scam defense category hooks - Ultra-fast version (1-2s response)."""

import json
import asyncio
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
import hashlib

from langchain.schema import Document

from agent.categories.base import CategoryHooks
from agent.retrievers.scam_retriever import get_scam_defense_retriever


# ========== ì„¤ì • ë° ìºì‹± ========== #

_BASE_DIR = Path(__file__).resolve().parents[2]
_REAL_TIME_DATA_PATH = _BASE_DIR / "data" / "scam_defense" / "scam_patterns.json"
_REAL_TIME_DATA_CACHE: Optional[Dict[str, Any]] = None
_QUERY_CACHE: Dict[str, Any] = {}  # ì¿¼ë¦¬ ê²°ê³¼ ìºì‹œ
_CACHE_SIZE_LIMIT = 100

_DANGER_LEVEL_ORDER = {
    "ë§¤ìš°ë†’ìŒ": 4,
    "ë†’ìŒ": 3,
    "ì¤‘ê°„": 2,
    "ë‚®ìŒ": 1,
    "ì •ë³´": 0,
    "high_risk": 3,
    "medium_risk": 2,
    "low_risk": 1,
}

MAX_PREVIEW = 400  # ì¶•ì†Œ
MAX_SOURCE_PREVIEW = 150  # ì¶•ì†Œ
MAX_SOURCES = 3  # ì¶•ì†Œ


# ========== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ========== #


@lru_cache(maxsize=2048)
def _digits_only(value: Optional[str]) -> str:
    """Extract and cache digits."""
    return "".join(ch for ch in (value or "") if ch.isdigit())


@lru_cache(maxsize=1)
def _load_real_time_data() -> Dict[str, Any]:
    """Load and cache scam dataset (ì‹±ê¸€í†¤)."""
    global _REAL_TIME_DATA_CACHE

    if _REAL_TIME_DATA_CACHE is not None:
        return _REAL_TIME_DATA_CACHE

    try:
        raw = _REAL_TIME_DATA_PATH.read_text(encoding="utf-8")
        _REAL_TIME_DATA_CACHE = json.loads(raw)
    except Exception:
        _REAL_TIME_DATA_CACHE = {}

    return _REAL_TIME_DATA_CACHE


def _hash_query(query: str, sender: Optional[str]) -> str:
    """ì¿¼ë¦¬ í•´ì‹œ ìƒì„± (ìºì‹œ í‚¤)."""
    key = f"{query}|{sender or ''}"
    return hashlib.md5(key.encode()).hexdigest()


def _clean_cache():
    """ìºì‹œ í¬ê¸° ì œí•œ."""
    global _QUERY_CACHE
    if len(_QUERY_CACHE) > _CACHE_SIZE_LIMIT:
        # ê°€ì¥ ì˜¤ë˜ëœ ì ˆë°˜ ì œê±°
        keys = list(_QUERY_CACHE.keys())
        for key in keys[: _CACHE_SIZE_LIMIT // 2]:
            _QUERY_CACHE.pop(key, None)


class ScamDefenseHooks(CategoryHooks):
    """ê¸ˆìœµ ì‚¬ê¸° íƒì§€ ì´ˆê³ ì† ë²„ì „ (1-2ì´ˆ ëª©í‘œ)

    ìµœì í™”:
    - ë³‘ë ¬ ì²˜ë¦¬ (RAG + íŒ¨í„´ ë¶„ì„ ë™ì‹œ ì‹¤í–‰)
    - ë‹¨ì¼ LLM í˜¸ì¶œ (Agent í†µí•©)
    - ì¿¼ë¦¬ ìºì‹±
    - ChromaDB ìµœì í™”
    """

    name: str = "scam_defense"
    web_search_enabled: bool = False
    top_k: int = 5  # ì¶•ì†Œ (ì†ë„â†‘)
    min_relevance_threshold: float = 0.5  # ìƒí–¥ (ì •í™•ë„ ìœ ì§€)

    _retriever: Any = None
    _executor: ThreadPoolExecutor = None

    # ========== í†µí•© í”„ë¡¬í”„íŠ¸ (ë‹¨ì¼ LLM í˜¸ì¶œ) ========== #

    rewrite_system_prompt: str = (
        "ì˜ì‹¬ ë©”ì‹œì§€ë¥¼ ì‚¬ê¸° íŒ¨í„´ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì¬ì‘ì„±í•˜ë¼.\n"
        "í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ (OTP, ê³„ì¢Œì´ì²´, ë³¸ì¸í™•ì¸, ì¹´ë“œì •ì§€, ë³´ì´ìŠ¤í”¼ì‹± ë“±)\n"
        "ì˜ˆ: 'KBì€í–‰ OTP ì•Œë ¤ì£¼ì„¸ìš”' â†’ 'ê¸ˆìœµê¸°ê´€ ì‚¬ì¹­ OTP ê°œì¸ì •ë³´ ìš”êµ¬'\n"
        "ì¿¼ë¦¬ë§Œ ë°˜í™˜."
    )

    # í†µí•© ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ (Agent 3ê°œ â†’ 1ê°œë¡œ í†µí•©)
    unified_system_prompt: str = (
        "ë„ˆëŠ” ê¸ˆìœµì‚¬ê¸° ë°©ì§€ ì „ë¬¸ ìƒë‹´ì‚¬ë‹¤. "
        "ì œê³µëœ Knowledge Base(RAG)ì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ "
        "êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ë¼.\n\n"
        "ë‹µë³€ êµ¬ì„±:\n"
        "1. ì‚¬ê¸° ì—¬ë¶€ íŒë‹¨ ë° ìœ„í—˜ë„ í‰ê°€ (ë§¤ìš°ë†’ìŒ/ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ)\n"
        "2. ì‚¬ê¸° ìœ í˜• ë° ìˆ˜ë²• ì„¤ëª…\n"
        "3. ì¦‰ì‹œ í•´ì•¼ í•  ëŒ€ì‘ ë°©ë²• (ìš°ì„ ìˆœìœ„ë³„ ë²ˆí˜¸ ëª©ë¡)\n"
        "4. ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  í–‰ë™ (ë²ˆí˜¸ ëª©ë¡)\n"
        "5. ì‹ ê³  ë°©ë²• ë° ì—°ë½ì²˜\n"
        "6. ì˜ˆë°© íŒ ë° ì£¼ì˜ì‚¬í•­\n"
        "7. ì°¸ê³ í•œ ì¶œì²˜ (Knowledge Base, ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰)\n\n"
        "ë‹µë³€ í˜•ì‹:\n"
        "- ìœ„í—˜ë„ ì•„ì´ì½˜ ì‚¬ìš© (ğŸš¨ ë§¤ìš°ìœ„í—˜, âš ï¸ ìœ„í—˜, âš¡ ì£¼ì˜, â„¹ï¸ ì•ˆì „)\n"
        "- ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸\n"
        "- ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©\n"
        "- ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì–»ì€ ìµœì‹  ì •ë³´ ìš°ì„  ë°˜ì˜\n\n"
        "ì •í™•í•œ ì¶œì²˜ ë¬¸ì„œë¥¼ ê·¼ê±°ë¡œ ë‹µë³€í•˜ë˜, ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²½ìš° ì‹ ì¤‘í•œ íƒœë„ë¥¼ ìœ ì§€í•˜ë¼."
    )

    answer_system_prompt: str = ""  # ì‚¬ìš© ì•ˆ í•¨ (unified_system_prompt ì‚¬ìš©)

    # ========== Helper ë©”ì„œë“œ (ê°„ì†Œí™”) ========== #

    @staticmethod
    def format_documents(documents: List[Document]) -> str:
        """ë¬¸ì„œ í¬ë§¤íŒ… (ê°„ì†Œí™”)."""
        if not documents:
            return "ì—†ìŒ"

        formatted = []
        for idx, doc in enumerate(documents[:5], 1):  # ìµœëŒ€ 5ê°œ
            meta = doc.metadata or {}
            label = meta.get("scam_type") or meta.get("title") or f"ë¬¸ì„œ{idx}"
            content = doc.page_content.strip()[:MAX_PREVIEW]  # ì¶•ì†Œ
            formatted.append(f"[{label}] {content}")

        return "\n\n".join(formatted)

    @staticmethod
    def prepare_sources(documents: List[Document]) -> List[Dict[str, Any]]:
        """ì¶œì²˜ ì¤€ë¹„ (ê°„ì†Œí™”)."""
        if not documents:
            return []

        sources = []
        for idx, doc in enumerate(documents[:MAX_SOURCES], 1):
            meta = doc.metadata or {}
            sources.append(
                {
                    "content": doc.page_content.strip()[:MAX_SOURCE_PREVIEW],
                    "metadata": {
                        "source": meta.get("scam_type")
                        or meta.get("title")
                        or f"doc{idx}",
                        "danger_level": meta.get("danger_level"),
                    },
                }
            )
        return sources

    # ========== ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„ (ìµœì í™”) ========== #

    def analyze_realtime_patterns(
        self, query: str, sender: Optional[str] = None
    ) -> Tuple[List[Document], Dict[str, Any]]:
        """ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„ (ìºì‹œ + ìµœì í™”)."""
        # ìºì‹œ í™•ì¸
        cache_key = _hash_query(query, sender)
        if cache_key in _QUERY_CACHE:
            return _QUERY_CACHE[cache_key]

        dataset = _load_real_time_data()
        if not dataset:
            result = ([], {})
            _QUERY_CACHE[cache_key] = result
            return result

        query_lower = query.strip().lower()
        if not query_lower:
            result = ([], {})
            _QUERY_CACHE[cache_key] = result
            return result

        sender_lower = (sender or "").strip().lower()
        query_digits = _digits_only(query)
        sender_digits = _digits_only(sender)

        pattern_docs = []
        scam_matches = []
        highest_score = -1
        highest_level = None

        # 1. ì‚¬ê¸° íŒ¨í„´ ë§¤ì¹­ (ìµœì í™”)
        for scam in dataset.get("financial_scams", [])[:20]:  # ìµœëŒ€ 20ê°œë§Œ
            patterns = [
                p for p in scam.get("patterns", []) if p and p.lower() in query_lower
            ]
            sender_patterns = [
                p
                for p in scam.get("sender_patterns", [])
                if p
                and (
                    p.lower() in query_lower
                    or (sender_lower and p.lower() in sender_lower)
                )
            ]

            if not patterns and not sender_patterns:
                continue

            scam_type = scam.get("type", "ì•Œ ìˆ˜ ì—†ìŒ")
            danger = scam.get("danger_level", "ì •ë³´")
            score = _DANGER_LEVEL_ORDER.get(danger, -1)

            if score > highest_score:
                highest_score = score
                highest_level = danger

            # ê°„ì†Œí™”ëœ ë¬¸ì„œ ìƒì„±
            content = f"ìœ í˜•: {scam_type} | ìœ„í—˜ë„: {danger}"
            if patterns:
                content += f"\níŒ¨í„´: {', '.join(patterns[:3])}"  # ìµœëŒ€ 3ê°œ

            pattern_docs.append(
                Document(
                    page_content=content,
                    metadata={
                        "source": "ì‹¤ì‹œê°„ì•Œë¦¼",
                        "scam_type": scam_type,
                        "danger_level": danger,
                        "origin": "web_search",
                    },
                )
            )

            scam_matches.append(
                {
                    "scam_type": scam_type,
                    "danger_level": danger,
                    "matched_patterns": patterns[:3],  # ì¶•ì†Œ
                }
            )

        # 2. í‚¤ì›Œë“œ ë§¤ì¹­ (ê°„ì†Œí™”)
        keyword_matches = {}
        for risk_level, keywords in (dataset.get("keywords") or {}).items():
            hits = [
                k for k in keywords[:10] if k and k.lower() in query_lower
            ]  # ìµœëŒ€ 10ê°œ
            if hits:
                keyword_matches[risk_level] = hits[:3]  # ì¶•ì†Œ
                score = _DANGER_LEVEL_ORDER.get(risk_level, -1)
                if score > highest_score:
                    highest_score = score
                    highest_level = risk_level

        # 3. ê³µì‹ ì—°ë½ì²˜ (ê°„ì†Œí™”)
        legitimate_matches = []
        for org, phone in list((dataset.get("legitimate_contacts") or {}).items())[
            :5
        ]:  # ìµœëŒ€ 5ê°œ
            norm_phone = _digits_only(phone)
            if (org and org.lower() in query_lower) or (
                norm_phone
                and (norm_phone in query_digits or norm_phone in sender_digits)
            ):
                legitimate_matches.append({"organization": org, "phone": phone})
                pattern_docs.append(
                    Document(
                        page_content=f"{org} ê³µì‹: {phone}",
                        metadata={"source": "ê³µì‹ì—°ë½ì²˜", "origin": "web_search"},
                    )
                )

        # ê²°ê³¼ ìš”ì•½
        pattern_analysis = {
            "query": query.strip()[:100],  # ì¶•ì†Œ
            "sender": (sender or "").strip()[:50],
            "risk_summary": {
                "highest_level": highest_level,
                "score": highest_score,
                "is_high_risk": highest_score >= 3,
            },
            "scam_matches": scam_matches[:5],  # ìµœëŒ€ 5ê°œ
            "keyword_matches": keyword_matches,
            "legitimate_contacts": legitimate_matches[:3],  # ìµœëŒ€ 3ê°œ
        }

        result = (pattern_docs[:5], pattern_analysis)  # ìµœëŒ€ 5ê°œ ë¬¸ì„œ

        # ìºì‹œ ì €ì¥
        _QUERY_CACHE[cache_key] = result
        _clean_cache()

        return result

    def _format_pattern_analysis(self, pattern: Dict[str, Any]) -> str:
        """íŒ¨í„´ í¬ë§¤íŒ… (ê°„ì†Œí™”)."""
        if not pattern:
            return "ë§¤ì¹­ ì—†ìŒ"

        lines = []
        risk = pattern.get("risk_summary", {})

        if level := risk.get("highest_level"):
            lines.append(f"ìœ„í—˜ë„: {level}")

        if matches := pattern.get("scam_matches", [])[:3]:  # ìµœëŒ€ 3ê°œ
            lines.append("ë§¤ì¹­ ìœ í˜•:")
            for m in matches:
                lines.append(f"- {m['scam_type']} ({m['danger_level']})")

        if kws := pattern.get("keyword_matches"):
            lines.append("ìœ„í—˜ í‚¤ì›Œë“œ:")
            for level, words in list(kws.items())[:2]:  # ìµœëŒ€ 2ê°œ
                lines.append(f"- {level}: {', '.join(words[:3])}")

        return "\n".join(lines) if lines else "ë§¤ì¹­ ì—†ìŒ"

    def get_web_documents(
        self, query: str, state: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """ì›¹ ë¬¸ì„œ ê²€ìƒ‰."""
        sender = state.get("sender") if state else None
        documents, _ = self.analyze_realtime_patterns(query, sender)
        return documents

    def get_retriever(self) -> Any:
        """Retriever ë°˜í™˜ (ìºì‹± + ìµœì í™”)."""
        if self._retriever is None:
            # ChromaDB ìµœì í™” ì„¤ì •
            self._retriever = get_scam_defense_retriever(
                config={
                    "k": self.top_k,
                    "search_type": "similarity",  # MMRë³´ë‹¤ ë¹ ë¦„
                    "fetch_k": self.top_k * 2,  # ë°°ì¹˜ ìµœì í™”
                }
            )
        return self._retriever

    # ========== ë³‘ë ¬ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ========== #

    def _get_executor(self) -> ThreadPoolExecutor:
        """Thread pool ì‹±ê¸€í†¤."""
        if self._executor is None:
            self._executor = ThreadPoolExecutor(max_workers=3)
        return self._executor

    def _parallel_retrieve(
        self, query: str, state: Optional[Dict[str, Any]]
    ) -> Tuple[List[Document], List[Document], Dict[str, Any]]:
        """ë³‘ë ¬ ê²€ìƒ‰: RAG + íŒ¨í„´ ë¶„ì„ ë™ì‹œ ì‹¤í–‰."""
        executor = self._get_executor()
        sender = state.get("sender") if state else None

        # ë³‘ë ¬ ì‹¤í–‰
        rag_future = executor.submit(self.get_retriever().get_relevant_documents, query)
        pattern_future = executor.submit(self.analyze_realtime_patterns, query, sender)

        # ê²°ê³¼ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ 1ì´ˆ)
        try:
            rag_docs = rag_future.result(timeout=1.0)
        except Exception:
            rag_docs = []

        try:
            pattern_docs, pattern_analysis = pattern_future.result(timeout=1.0)
        except Exception:
            pattern_docs, pattern_analysis = [], {}

        return rag_docs, pattern_docs, pattern_analysis

    # ========== ë‹¨ì¼ LLM í˜¸ì¶œ (í†µí•© Agent) ========== #

    def _generate_unified_answer(
        self,
        query: str,
        rag_docs: List[Document],
        pattern_docs: List[Document],
        pattern: Dict[str, Any],
    ) -> str:
        """í†µí•© ë‹µë³€ ìƒì„± (ë‹¨ì¼ LLM í˜¸ì¶œ)."""
        # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        rag_ctx = self.format_documents(rag_docs) if rag_docs else "ê´€ë ¨ ì •ë³´ ì—†ìŒ"
        pattern_ctx = self._format_pattern_analysis(pattern)

        # ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ í†µí•©
        user_content = (
            f"**ì˜ì‹¬ ë©”ì‹œì§€:**\n{query}\n\n"
            f"**ë°œì‹ ì:** {pattern.get('sender', 'ë¯¸ì œê³µ')}\n\n"
            f"**ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„:**\n{pattern_ctx}\n\n"
            f"**Knowledge Base (ê³¼ê±° ì‚¬ë¡€):**\n{rag_ctx}\n\n"
            f"**ì‹¤ì‹œê°„ ì‚¬ê¸° DB:**\n{self.format_documents(pattern_docs) if pattern_docs else 'ë§¤ì¹­ ì—†ìŒ'}\n\n"
            "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¦‰ì‹œ ëŒ€ì‘ ê°€ì´ë“œë¥¼ ì‘ì„±í•˜ë¼."
        )

        # LLM í˜¸ì¶œ (ë‹¨ì¼)
        messages = [
            {"role": "system", "content": self.unified_system_prompt},
            {"role": "user", "content": user_content},
        ]

        try:
            response = self.llm.invoke(messages)
            return response.content.strip()
        except Exception as e:
            return f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\nê¸´ê¸‰ ìƒë‹´: ê²½ì°°ì²­ 182 | ê¸ˆìœµê°ë…ì› 1332"

    # ========== ë©”ì¸ ì§„ì…ì  ========== #

    def generate_answer(self, query, documents, web_documents, state):
        """
        ì´ˆê³ ì† ë‹µë³€ ìƒì„± (ëª©í‘œ: 1-2ì´ˆ)

        ìµœì í™”:
        1. ë³‘ë ¬ ê²€ìƒ‰ (RAG + íŒ¨í„´)
        2. ë‹¨ì¼ LLM í˜¸ì¶œ (Agent í†µí•©)
        3. ìºì‹± ì „ëµ
        4. ë¬¸ì„œ ì¶•ì†Œ
        """
        import time

        start = time.time()

        print(f"[INFO] ğŸš€ ì‚¬ê¸° íƒì§€ ì‹œì‘")

        # 1. ë³‘ë ¬ ê²€ìƒ‰ (RAG + íŒ¨í„´ ë¶„ì„)
        rag_docs, pattern_docs, pattern_analysis = self._parallel_retrieve(query, state)

        print(
            f"[INFO] â±ï¸ ê²€ìƒ‰ ì™„ë£Œ: {time.time() - start:.2f}s | RAG: {len(rag_docs)}, íŒ¨í„´: {len(pattern_docs)}"
        )

        # 2. ë‹¨ì¼ LLM í˜¸ì¶œ (í†µí•© Agent)
        answer = self._generate_unified_answer(
            query, rag_docs, pattern_docs, pattern_analysis
        )

        # 3. ì¶œì²˜ ì¤€ë¹„
        all_docs = (rag_docs or []) + (pattern_docs or [])
        sources = self.prepare_sources(all_docs)

        elapsed = time.time() - start
        print(f"[INFO] âœ… ì™„ë£Œ: {elapsed:.2f}s")

        return {
            "answer": answer,
            "sources": sources,
            "pattern_analysis": pattern_analysis,
            "elapsed_time": elapsed,
        }


__all__ = ["ScamDefenseHooks"]
